{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d578be4-34c2-47e4-87ec-12cfe11462d1",
   "metadata": {},
   "source": [
    "## DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08801ed-b855-42bf-9c34-30fcd5740c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f719b184-6675-4710-bdb8-69fdd5e8d5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (4.48.1)\n",
      "Requirement already satisfied: torch in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vaiju\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f0dde58-02aa-45eb-824f-e975ec4823be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ") \n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045b33e-2987-402d-a92c-49e308b5e213",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d6d0c0-5fed-420d-9275-08496379221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'complaints.csv' \n",
    "df = pd.read_csv(filename, nrows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "257908a2-4c41-49e0-9170-f4f5bf2c2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining columns and dropping NaNs\n",
    "feature_column = 'Consumer complaint narrative'\n",
    "label_column = 'Product'\n",
    "df.dropna(subset=[feature_column], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac978cdf-3fcf-4081-bb6c-dbfe764063d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Duplicate Categories\n",
    "credit_categories = [\n",
    "    'Credit reporting, credit repair services, or other personal consumer reports',\n",
    "    'Credit reporting or other personal consumer reports',\n",
    "    'Credit reporting'\n",
    "]\n",
    "clean_name = 'Credit Reporting' \n",
    "df[label_column] = df[label_column].replace(credit_categories, clean_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f46a2c18-45cc-47f8-91ad-16d182378a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the \"xxxx\" redactions from X\n",
    "X = df[feature_column]\n",
    "y = df[label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de5bf386-6b66-417a-9281-bf5748afb061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data size: 6867\n",
      "Total categories: 16\n"
     ]
    }
   ],
   "source": [
    "# Removing rare classes\n",
    "class_counts = y.value_counts()\n",
    "rare_classes = class_counts[class_counts < 2].index.tolist()\n",
    "if len(rare_classes) > 0:\n",
    "    keep_indices = y.isin(rare_classes) == False\n",
    "    X_filtered = X[keep_indices]\n",
    "    y_filtered = y[keep_indices]\n",
    "else:\n",
    "    X_filtered = X\n",
    "    y_filtered = y\n",
    "\n",
    "print(f\"Filtered data size: {len(y_filtered)}\")\n",
    "print(f\"Total categories: {y_filtered.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3566ba8-31c2-40c3-aa9b-a01e770e318b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data prepared with integer labels:\n",
      "                                                  text        label_name  \\\n",
      "62                          These are not my accounts.  Credit Reporting   \n",
      "94   For the past few years I was in and out of hot...   Debt collection   \n",
      "117  Kindly address this issue on my credit report....  Credit Reporting   \n",
      "120  I AM FORMALLY REQUESTING THE IMMEDIATE REMOVAL...   Debt collection   \n",
      "216  There are XXXX collections being reported to t...  Credit Reporting   \n",
      "\n",
      "     label  \n",
      "62       3  \n",
      "94       6  \n",
      "117      3  \n",
      "120      6  \n",
      "216      3  \n",
      "\n",
      "Example label mapping: 'Credit Reporting' is 3\n"
     ]
    }
   ],
   "source": [
    "# Creating Label Dictionaries as transformer models don't work with strings\n",
    "# Converting \"Mortgage\" -> 0, \"Debt collection\" -> 1, etc.\n",
    "\n",
    "# sorted list of unique category names\n",
    "labels = sorted(y_filtered.unique())\n",
    "\n",
    "# Creating a dictionary to map name to ID (e.g., 'Credit Reporting': 0)\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Creating a reverse dictionary to map ID back to name (e.g., 0: 'Credit Reporting')\n",
    "id_to_label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "# Adding the integer labels to our dataframe\n",
    "df_final = pd.DataFrame({\n",
    "    'text': X_filtered,\n",
    "    'label_name': y_filtered\n",
    "})\n",
    "df_final['label'] = df_final['label_name'].map(label_to_id)\n",
    "\n",
    "print(\"\\nData prepared with integer labels:\")\n",
    "print(df_final.head())\n",
    "print(f\"\\nExample label mapping: 'Credit Reporting' is {label_to_id['Credit Reporting']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7448b9a8-43f8-4989-86c5-1044dda93c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set size: 5493, Validation set size: 1374\n"
     ]
    }
   ],
   "source": [
    "# Splitting data before tokenizing\n",
    "train_df, val_df = train_test_split(\n",
    "    df_final,\n",
    "    test_size=0.2,\n",
    "    stratify=df_final['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(train_df)}, Validation set size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516bdb9-d737-4bec-a83e-fa8fcac38108",
   "metadata": {},
   "source": [
    "# Load the tokenizer and tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9f4aec8-7231-4d0c-a7f3-78a65c587ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ec4b1baa1d4f39aeb48632e8e1752c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaiju\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vaiju\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a4375c7cf24734b8415eb22f8ba7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2397ffecda49f0a6df58d225ae7877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa765fc162874ff6af6b50d1e7152167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer for distilbert-base-uncased.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded tokenizer for {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "484bfd4a-5c4b-440e-a118-e0cb327d61ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted DataFrames to Datasets:\n",
      "Dataset({\n",
      "    features: ['text', 'label_name', 'label', '__index_level_0__'],\n",
      "    num_rows: 5493\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "print(f\"Converted DataFrames to Datasets:\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1744e05-7a3c-4baf-a2ab-9e9c574ebcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # This tokenizes the text. We truncate long complaints as we planned in EDA\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        padding='max_length', # Pad shorter complaints to the max length\n",
    "        truncation=True,      # Truncate complaints longer than the max length\n",
    "        max_length=512        # Our chosen max length from the EDA\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33829ecb-cdc2-491e-aaad-8c2778ccbe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70d491380f247b780306641e35b77fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5493 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69e96c3c48d4fc694df2a652e3414d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1374 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying the tokenizer to all entries in our datasets\n",
    "print(\"\\nTokenizing training data...\")\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Tokenizing validation data...\")\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0710459-2085-4c0d-9d90-b1fe28915fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the datasets\n",
    "# The model only needs 'input_ids', 'attention_mask', and 'label'.\n",
    "# Removing the text columns to save memory.\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns(\n",
    "    ['text', 'label_name', '__index_level_0__']\n",
    ")\n",
    "tokenized_val_dataset = tokenized_val_dataset.remove_columns(\n",
    "    ['text', 'label_name', '__index_level_0__']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fa570d9-1250-41f2-9a0e-4d81e5c94c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'label' to 'labels' because the model expects this exact name\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_val_dataset = tokenized_val_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ba184c4-023e-4a69-a7c3-8610623a90d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization complete. Final training dataset features:\n",
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5493\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Set the format to 'torch' so it returns PyTorch tensors\n",
    "tokenized_train_dataset.set_format('torch')\n",
    "tokenized_val_dataset.set_format('torch')\n",
    "\n",
    "print(\"\\nTokenization complete. Final training dataset features:\")\n",
    "print(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ade48-37bb-48f4-9097-c131579276a1",
   "metadata": {},
   "source": [
    "## Load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb2f32b4-a70c-4068-b8e8-7acd2ac146d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 16\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(labels) \n",
    "print(f\"Number of unique labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81265eaa-faaf-47b0-a28b-55c9e80b6b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bba8165c38a4cac9b8cb21667cdd51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded distilbert-base-uncased with a 16-class head.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id_to_label,  \n",
    "    label2id=label_to_id   \n",
    ")\n",
    "\n",
    "print(f\"Successfully loaded {model_name} with a {num_labels}-class head.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641b0d8-89b6-49e5-a9f7-3d15bad1e196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
